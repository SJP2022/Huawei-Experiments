{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b4eec2",
   "metadata": {},
   "source": [
    "# 机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004bb8b",
   "metadata": {},
   "source": [
    "### 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3970f1b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import io\n",
    "import time\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b694ab",
   "metadata": {},
   "source": [
    "### 指定数据路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62ebb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"data/cmn.txt\" \n",
    "## 数据集文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad139e6",
   "metadata": {},
   "source": [
    "### 定义预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00002c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng(w):\n",
    "    w = w.lower().strip()\n",
    "    # 单词和标点之间加空格\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    # 多个空格合并为一个\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # 除了(a-z, A-Z, \".\", \"?\", \"!\", \",\")这些字符外，全替换成空格\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    # 增加开始结束标志，让模型知道何时停止预测\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "def preprocess_chinese(w):\n",
    "    w = w.lower().strip()\n",
    "    w = jieba.cut(w, cut_all=False, HMM=True)\n",
    "    w = \" \".join(list(w)) # 词之间增加空格\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a87701",
   "metadata": {},
   "source": [
    "### 预处理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72676592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.652 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 我 可以 借 这 本书 吗 ？ <end>\n"
     ]
    }
   ],
   "source": [
    "en_sentence = \"May I borrow this book?\"\n",
    "chn_sentence = \"我可以借这本书吗？\"\n",
    "print(preprocess_eng(en_sentence))\n",
    "print(preprocess_chinese(chn_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640ccea",
   "metadata": {},
   "source": [
    "### 加载数据集，并进行预处理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6adcd90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> hi . <end>', '<start> 嗨 。 <end>'],\n",
       " ['<start> hi . <end>', '<start> 你好 。 <end>'],\n",
       " ['<start> run . <end>', '<start> 你 用 跑 的 。 <end>'],\n",
       " ['<start> wait ! <end>', '<start> 等等 ！ <end>'],\n",
       " ['<start> hello ! <end>', '<start> 你好 。 <end>'],\n",
       " ['<start> i try . <end>', '<start> 让 我 来 。 <end>'],\n",
       " ['<start> i won ! <end>', '<start> 我 赢 了 。 <end>'],\n",
       " ['<start> oh no ! <end>', '<start> 不会 吧 。 <end>'],\n",
       " ['<start> cheers ! <end>', '<start> 乾杯 ! <end>'],\n",
       " ['<start> he ran . <end>', '<start> 他 跑 了 。 <end>'],\n",
       " ['<start> hop in . <end>', '<start> 跳进来 。 <end>'],\n",
       " ['<start> i lost . <end>', '<start> 我 迷失 了 。 <end>'],\n",
       " ['<start> i quit . <end>', '<start> 我 退出 。 <end>'],\n",
       " ['<start> i m ok . <end>', '<start> 我 沒事 。 <end>'],\n",
       " ['<start> listen . <end>', '<start> 听 着 。 <end>'],\n",
       " ['<start> no way ! <end>', '<start> 不 可能 ！ <end>'],\n",
       " ['<start> no way ! <end>', '<start> 没门 ！ <end>'],\n",
       " ['<start> really ? <end>', '<start> 你 确定 ？ <end>'],\n",
       " ['<start> try it . <end>', '<start> 试试 吧 。 <end>'],\n",
       " ['<start> we try . <end>', '<start> 我们 来 试试 。 <end>']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据，每个元素的样式是 [英文, 中文]\n",
    "def create_dataset(path, num_examples=None):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    word_pairs = [[preprocess_eng(w[0]), preprocess_chinese(w[1])]\n",
    "                    for w in word_pairs]\n",
    "    return word_pairs\n",
    "word_pairs = create_dataset(path_to_file)\n",
    "# 展示前 20 个数据\n",
    "word_pairs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6995225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if a person has not had a chance to acquire his target language by the time he s an adult , he s unlikely to be able to reach native speaker level in that language . <end>\n",
      "<start> 如果 一個 人 在 成人 前 沒 有 機會習 得 目標 語言 ， 他 對 該 語言 的 認識 達 到 母語者 程度 的 機會 是 相當 小 的 。 <end>\n",
      "20403 20403\n"
     ]
    }
   ],
   "source": [
    "en, chn = zip(*create_dataset(path_to_file))\n",
    "print(en[-1])\n",
    "print(chn[-1])\n",
    "# 显示数据大小\n",
    "print(len(en), len(chn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a11c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    # 取数据中的最大文本长度，用来将所有文本统一成一致的长度，模型才能够正常训练\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    \"\"\"\n",
    "    1. 分词\n",
    "    2. 转换成 id\n",
    "    3. padding, 将每个句子统一成相同的长度，长度不足的后面补 0\n",
    "    \"\"\"\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='')\n",
    "    # 生成 词和 id 的映射词典 {word:id}\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # 将词转换成对应的 id\n",
    "    text_ids = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # 统一成相同的长度\n",
    "    padded_text_ids = tf.keras.preprocessing.sequence.pad_sequences(text_ids,\n",
    "                                                                padding='post')\n",
    "    return padded_text_ids, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    # 加载数据，并做预处理\n",
    "    # 将中文设置为源语言，英文设置为目标语言\n",
    "    targ_lang, inp_lang = zip(*create_dataset(path, num_examples))\n",
    "    input_data, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_data, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_data, target_data, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf7d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19382 19382 1021 1021\n"
     ]
    }
   ],
   "source": [
    "# num_examples 设置训练数据的大小\n",
    "# num_examples = 10000, 如果num examples = None 则表示不限制大小，所有样本用于训练\n",
    "num_examples = None\n",
    "input_data, target_data, inp_lang, targ_lang = load_dataset(\n",
    "    path_to_file, num_examples)\n",
    "# 计算中文数据和英文数据中的最大长度\n",
    "max_length_targ, max_length_inp = max_length(\n",
    "    target_data), max_length(input_data)\n",
    "# 分割训练数据和验证数据\n",
    "input_train, input_val, target_train, target_val = train_test_split(\n",
    "    input_data, target_data, test_size=0.05)\n",
    "# 显示训练数据和验证数据的大小\n",
    "print(len(input_train), len(target_train),\n",
    "        len(input_val), len(target_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f1a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：源语言：中文， 词和 id 的映射关系\n",
      "1 ----> <start>\n",
      "8 ----> 他\n",
      "82 ----> 昨天\n",
      "818 ----> 上学\n",
      "707 ----> 迟到\n",
      "6 ----> 了\n",
      "3 ----> 。\n",
      "2 ----> <end>\n",
      "\n",
      "输出：目标语言：英文， 词和 id 的映射关系\n",
      "1 ----> <start>\n",
      "11 ----> he\n",
      "26 ----> was\n",
      "176 ----> late\n",
      "27 ----> for\n",
      "90 ----> school\n",
      "123 ----> yesterday\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "## 查看 词和 id 的对应关系\n",
    "def convert(lang, data):\n",
    "    for t in data:\n",
    "        if t != 0:\n",
    "            print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "print(\"输入：源语言：中文， 词和 id 的映射关系\")\n",
    "convert(inp_lang, input_train[0])\n",
    "print()\n",
    "print(\"输出：目标语言：英文， 词和 id 的映射关系\")\n",
    "convert(targ_lang, target_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae3fb8",
   "metadata": {},
   "source": [
    "### 转换成 tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3ea2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 32]), TensorShape([64, 38]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# 0 是为 padding 保留的一个特殊 id， 所以要 + 1\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "\n",
    "# 先做 shuffle， 再取 batch\n",
    "# see https://stackoverflow.com/questions/50437234/tensorflow-dataset-shuffle-then-batch-or-batch-then-shuffle\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (input_train, target_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c79ce",
   "metadata": {},
   "source": [
    "### 定义 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c21a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=vocab_inp_size=9394, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                            return_sequences=True,\n",
    "                                            return_state=True,\n",
    "                                            #recurrent_activation='sigmoid',\n",
    "                                            recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        # x 是训练数据，shape == (batch_size，max_length) -> (64, 46)\n",
    "        # embedding 后得到每个词的词向量, x shape == (batch_size, max_length, embedding_dim) -> (64, 46, 256)\n",
    "        x = self.embedding(x)\n",
    "        # 在 GRU 中，每一个时间步，输出层和隐藏层是相等的\n",
    "        # output 是所有时间步的输出层输出 shape == (batch_size, max_length, units) -> (64, 46, 1024)\n",
    "        # state 是最后一个时间步的隐藏层输出, shape == (batch_size, units) -> (64, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        # 初始化 gru 的隐层参数, shape == (batch_size, units) -> (64,1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05fb98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder 输出的维度: (batch size, sequence length, units) (64, 32, 1024)\n",
      "Encoder 隐层的维度: (batch size, units) (64, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# encoder 示例输出\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder 输出的维度: (batch size, sequence length, units) {}'.format(\n",
    "    sample_output.shape))\n",
    "print('Encoder 隐层的维度: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# GRU,在每一个时间步，隐层和输出层是相等的\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e8446",
   "metadata": {},
   "source": [
    "### 定义 Attention 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afb8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query shape == (batch_size, hidden size)\n",
    "        # 扩展时间维度 shape == (batch_size, 1, hidden size)\n",
    "        # 为了计算后面的 score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # score 维度为 1 是因为应用了 self.V, V 的维度是 1\n",
    "        # 应用 self.V 前后的维度是 (batch_size, max_length, units) --> (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        # 使用 softmax 得到 attention 的权重， attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # context_vector shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        # 相加后的 attention 上下文向量的维度：shape context_vector == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f14a4b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 输出的维度: (batch size, units) (64, 1024)\n",
      "Attention 权值参数的维度: (batch_size, sequence_length, 1) (64, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(\n",
    "    sample_hidden, sample_output)\n",
    "print(\"Attention 输出的维度: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention 权值参数的维度: (batch_size, sequence_length, 1) {}\".format(\n",
    "attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e9b3f",
   "metadata": {},
   "source": [
    "### 定义 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff301cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        # vocab_size=vocab_tar_size=6082, embedding_dim=256, dec_units=1024, batch_sz=64\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                            return_sequences=True,\n",
    "                                            return_state=True,\n",
    "                                            recurrent_initializer='glorot_uniform')\n",
    "        # 输出的维度是目标语言词汇表的大小，返回的是 softmax 概率，词汇表中每一个词的概率\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        # attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # This function outputs a result at each timestamp\n",
    "        # 计算 decoder 的第一个隐状态和 encoder 所有输入之间的 attention 权重， 得到上下文向量,context_vector\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        # embedding 后的维度 == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # 把上下文向量 context_vector 和 输入 embedding 拼接在一起\n",
    "        # context_vector shape == (batch_size, units) -> (64, 1024)\n",
    "        # 拼接后的数据维度 == (batch_size, 1, embedding_dim + hidden_size) -> (64, 1, 1024 + 256)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # 把拼接后的向量输入 gru\n",
    "        # 得到当前时间步的输出和隐状态\n",
    "        # output shape == (batch_size, 1, units) -> (64, 1, 1024), state shape == (batch_size, units) -> (64,1024)\n",
    "        output, state = self.gru(x)\n",
    "        # output shape == (batch_size, hidden_size=1024)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape == (batch_size, vocab) -> (64, 6082)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414abf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder 输出的维度: (batch_size, vocab size) (64, 6095)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                        sample_hidden, sample_output)\n",
    "print('Decoder 输出的维度: (batch_size, vocab size) {}'.format(\n",
    "sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d50bd1",
   "metadata": {},
   "source": [
    "### 定义优化器和损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd8ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"Calculate the loss value\n",
    "    Args:\n",
    "        real: the true label shape == (batch_size,) -> (64,)\n",
    "        pred: the probability of each word from the vocabulary, is the output from the decoder \n",
    "                shape == (batch_size, vocab_size) -> (64, 6082)\n",
    "    Returns:\n",
    "        the average loss of the data in a batch size\n",
    "    \"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7cbfc",
   "metadata": {},
   "source": [
    "### 设置 checkpoint 保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c2e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints/chinese-eng'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                    encoder=encoder,\n",
    "                                    decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148347d9",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aabd2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        # feed the <start> as the first input of the decoder\n",
    "        # dec input shape == (batch_size, 1) -> (64, 1)\n",
    "        dec_input = tf.expand_dims(\n",
    "            [targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # because of the data preprocessing(add a start token to the sentence)\n",
    "        # the first word is <start>, so t starts from 1(not 0)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(\n",
    "                dec_input, dec_hidden, enc_output)\n",
    "            # targ[:, t] is the true label(index of the word) of every sentence(in a batch) at the currenttimestamp\n",
    "            # like [ 85 18 25 25 ··· 1047 79 13], shape == (batch_size,) -> (64,)\n",
    "            # predictions shape == (batch_size, vocab_size) -> (64, 6082)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb287a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0463\n",
      "Epoch 1 Batch 100 Loss 1.0839\n",
      "Epoch 1 Batch 200 Loss 1.0337\n",
      "Epoch 1 Batch 300 Loss 0.8669\n",
      "Epoch 1 Loss 1.0609\n",
      "Epoch 2 Batch 0 Loss 0.8485\n",
      "Epoch 2 Batch 100 Loss 0.8291\n",
      "Epoch 2 Batch 200 Loss 0.8267\n",
      "Epoch 2 Batch 300 Loss 0.7402\n",
      "Epoch 2 Loss 0.8145\n",
      "Time taken for 1 epoch 2461.55344748497 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2 # 50 测试需要，设置训练轮数为 2， 实际为保证效果，建议设置为 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    # 获取 gru 的初始状态\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                    batch,\n",
    "                                                    batch_loss.numpy()))\n",
    "    \n",
    "    # 每两个迭代保存一次模型\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                            total_loss / steps_per_epoch))\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53eb2a6",
   "metadata": {},
   "source": [
    "### 定义测试和可视化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60c826fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \"\"\"Translate a sentence\n",
    "    \n",
    "    Args:\n",
    "        sentence: the test sentence \n",
    "    \"\"\"\n",
    "    \n",
    "    # max_length_targ 38, max_length_inp 64\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_chinese(sentence)\n",
    "    # convert each word to the index in the test sentence\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                            maxlen=max_length_inp,\n",
    "                                                            padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    # hidden shape == (1, 1024)\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    # enc out shape == (1, max_length_inp, 1024) -> (1, 46, 1024)\n",
    "    # enc hidden shape == (1, 1024)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                            dec_hidden,\n",
    "                                                            enc_out)\n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        # print(attention_weights)\n",
    "        # get the index which has the highest probability\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        # convert the index to the word\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        # when the decoder predicts the end, stop prediction\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        # the predicted id is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    # maybe you need to change the fname based on your system, so that the Chinese can be displayedin the plot\n",
    "    # font = FontProperties(fname=r\"C:\\\\WINDOWS\\\\Fonts\\\\simsun.ttc\", size=14)\n",
    "    # set the size of the plot\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # cmap means color map, viridis means blue-green-yellow\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    # set the x-tick/y-tick labels with list of string labels\n",
    "    # ax.set_xticklabels([''] + sentence, fontdict=fontdict, fontproperties=font)\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict)\n",
    "    # ax.set_yticklabels([''] + predicted_sentence, fontproperties=font, fontdict=fontdict)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    # set tick locators\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(\n",
    "        result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e94ba",
   "metadata": {},
   "source": [
    "### 离线加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad9a03f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/chinese-eng/ckpt-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb417110ca0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = 'checkpoints/chinese-eng'\n",
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c67f98",
   "metadata": {},
   "source": [
    "### 单句翻译测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0256b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> 我 有 一只 猫 <end>\n",
      "Predicted translation: i m a car . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6057/1884495568.py:56: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict)\n",
      "/tmp/ipykernel_6057/1884495568.py:58: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAJGCAYAAAD2wpepAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZd0lEQVR4nO3dfbBtB1nf8d9DbnJjQIKFIBAEI1aMoiJcBQatRJyCFJ3OiFZESkS9jq9QpVpaQKyCyIuSsSpcKhUcLSgtxbcp8mqUSiXYUaO8BY0SaICYSF6ES0Ke/rF36OFwL7nncp+77j7n85k5M9l7rb32sxebs79Za52d6u4AAJxot1l6AABgdxIZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAu0RVPamqLl96jlvsW3qAvaqqfiXJnbr7kSdoe29Mcml3/8CJ2N4mqaqvSfLCJB85wuK3d/e3nuSRNoL9dutubR8lOS/J/iMsOyvJ13b3FYPjbQzvtb1LZJxgVXX7JLfp7n84Sc+3L8nHPsXye3T3352MWRb0GUle1t1P33pnVZ2Z5H8uMtFmsN9u3a3to+7u+25/UFW9LH6/buW9lpP/+bB+znOSXNfdRwq8cU6XnABVdVpVPayqfj3JlUm+bH3/91TVO6vqI1V1VVW9uqr2VdXTkzwuyb+oql7/PGT9mGdV1Tuq6sNVdXlVPXv9f8RbnuvpVXVpVV1YVe9OcjjJbyb5miTfv2V7n7t+yN9U1euq6nFVdbuTtEsAyKf8fDi7qg5V1Qeq6rqq+oOqOrDlcRdW1fVV9dD17/wbquoNVXXetu3/aFVduV73pUm2/55/RJIr18/14OGX+0lExqehqr64qp6d5D1JXp7khiQPT3Lx+s3yC0l+Ism9kzw0/7/Yn5vkN5K8Nsld1z//a73shiSPT3J+ku9L8q1J/sO2pz4vybcl+eas3rDfmeSPk/yXLdt7z3rdL1ov+4ms3mgvqaqvrSr/2wMMuZXPh0ryu0nOTfLIJF+e5OIkr6+qu27ZzP4kT87qM+FBSe6Q5AVbnuNbkvxUkh9Pcr8k70jyw9tG+bWsPi8+M8lrquqyqnra9liZ4nDeDlXVHZM8JqsjEV+SVTg8Iclvbz0cVVX3yOpN9VvdfV2Sv03yZ+vF11fVh5Mc7u4rt26/u39yy83Lq+qZSZ6U5Klb7j8jyWO7+/1bnu+jSf7xCNt7R5KnVNVTk/yzJI9N8t+TXFtVv5rkJd39zuPbGwDcYgefD1+b5L5JzunuD6/vfmpVfUNWv6Ofvb5vX5LvX/8eT1U9N8mLq6p69R8ee2JWv8NfuF7/GVV1QZLPv+W5uvumJL+X5PfWp2setX6Op1XVHyV5aZLf6O7rT+jOWPNvszv3g0kuyuoCpi/o7m/s7t88wvmu12QVFn9TVb+2Pl3xmbe28ap6VFX90S2Hv5L8XJJ7bFvtiq2BcSx65Q+6+7uyqufXJPn3SQ7tZDsAHNWxfj7cP6uLgz+4Ps1x/fr3/X2S3GvLeodvCYy192X1L5mftb59flZHqrfafvvjuvva7n5xd1+Q5CuSfHaSX84qPEaIjJ07lOQpSe6U5NKq+tWq+udVddrWldZHL+6X5FuS/F1Wh7zeXlV3O9qGq+qBSV6W5NVJviGrQ2hPSXL6tlVvOJ7Bq+q+VfW8JO9K8o1Jfj6rygbg03dMnw9Zffa+P6ujGVt/vjCfeNT6pm2Pu+U/m35cn91Vtb+qvqmqXpnkzUk+nNXRkFcdz/aOhcjYoe5+X3c/o7vvneTrklyfVRhcUVXPq6r7bln3pu5+fXc/OcmXJrltVuffkuSjSba/8R6c5L3d/ZPd/ZbufleSex7jaEfaXqrq7usLg/4iyf/O6nqO70tyt+7+oe7+s+2PAWDndvD58KdZHUW4ubsv2/bzgR085duSPHDbfZ9wu1a+qqpemNWFpz+f5LIk9+/u+3X3Rd19zY5f7DFyTcanobvfnOTNVfXErI48PC7JW9bn287O6rDXxUmuTnJBVhfevG398MuTfH1V3TvJ3yf5UJJ3Jjm3qh6T1SGvhyV59DGOc3mSr1z/Vcn1Sa7u7puzOmXz1qwuFvqv3X318b9iAI7FrXw+vDbJm5K8qqp+NKvvXLlLVheGvra7//AYn+aiJC+tqrckeWNWpz0ekNVnzi2+PavvKHlVVp8nr+nuo37twYkmMk6A7j6c5BVJXlFVd87qeyvOT/Ivkzwtq3Nv707yXVvePC9K8pAkl2T1J0cXdPdvV9Vzkjw/q78r//3143/xGMZ4bpKXJPmr9WPPyyo8vri73/7pvkYAdu5Inw/d3VX1iKz+MuRFSe6c1emTN2V1IeaxbvvlVfV5SZ6R1efMbyX52SQXblntdUnu0t3XnoCXs2O1ukAVNldVPTzJA4/2RT/d/ZAl5jrV2W+37tb2UZIzu3v74epbvozr33X35SdjzlOd99re5ZoMAGCE0yXsBh9K8siqOtJ/B+atJ3uYDWK/3bpb20f3rKpLjvLYw3NjbRzvtT3K6RIAYITTJQDACJEBAIwQGQDACJFxCquqg0vPsInst52zz46P/XZ87Led29R9JjJObRv5pjoF2G87Z58dH/vt+NhvO7eR+0xkAAAj9vyfsJ5R+/vM3HbpMY7oxhzO6dm/9Bgbx37bOfvs+Nhvx8d+27lTeZ9dl2uu6u5zjrRsz38Z15m5bR5QD116DADYSK/tV/zt0ZY5XQIAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMCIXRsZVfUrVfU7S88BAHvVvqUHGPSEJLX0EACwV+3ayOjuDy09AwDsZU6XAAAjdm1kAADL2rWnSz6VqjqY5GCSnJmzFp4GAHanPXkko7sPdfeB7j5wevYvPQ4A7Ep7MjIAgHkiAwAYITIAgBEiAwAYsWv/uqS7L1x6BgDYyxzJAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABGiAwAYITIAABG7Ft6gKXVmftz2ud+/tJjbJSrD9xp6RE20puf84KlR9hIX/nk7116hI1zzuvfs/QIm+nGG5eeYDP936MvciQDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAESIDABghMgCAERsZGVX1xqr6pap6XlVdXVUfrKonVNX+qvqFqvqHqvq7qnrs0rMCwF61kZGx9pgk1yV5QJJnJXl+kv+R5J1JDiR5SZL/XFV3XWg+ANjTNjky/rK7n97d70rys0muSnJjd1/U3Zcl+Y9JKsmDtz+wqg5W1SVVdclHb/rHkzs1AOwRmxwZf37LP3R3J/lAkr/Yct+NSa5JcuftD+zuQ919oLsPnLHvrJMxKwDsOZscGTduu91HuW+TXyMAbCwfwADACJEBAIwQGQDAiH1LD3A8uvshR7jvPke47y4nZSAA4JM4kgEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjNi39ABLu/n003L47mcvPcZGOfvd/7j0CBvpgu/4rqVH2Eg33quWHmHj/P3X3H3pETbSGdffvPQIm+mVR1/kSAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMGLXREZVPbyq/rCqrqmqq6vq1VV1/tJzAcBetWsiI8ltkzw/yVcmeUiSDyX57ao6Y8GZAGDP2rf0ACdKd/+3rber6juSXJtVdPzRtmUHkxxMkv1n3uEkTQgAe8uuOZJRVfeqql+vqndX1bVJ3p/V67vH9nW7+1B3H+juA6efftuTPisA7AW75khGkt9JckWS70ny3iQ3JfmrJE6XAMACdkVkVNUdk3xhku/r7jes77tfdsnrA4BNtFs+hK9JclWS766q9yQ5N8lzsjqaAQAsYFdck9HdNyf5V0m+NMmlSX4hyVOTHF5yLgDYy3bLkYx09+uT3Gfb3bdbYhYAYJccyQAATj0iAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBH7lh5gabf58OHs/7PLlx5jo9x83fVLj7CRPuOss5YeYSNd9SXnLz3Cxrnhc2rpETZS12lLj7CZXnn0RY5kAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMEJkAAAjRAYAMGLXR0ZV7auqWnoOANhrTsnIqJUfqap3VdXhqrqiqn56vexZVfWOqvpwVV1eVc+uqjO3PPbpVXVpVV1YVe9OcjjJbZd6LQCwV+1beoCjeGaS703yw0kuTnJOki9fL7shyeOTvDfJFyV5QVYh8dQtjz8vybcl+eYkH03ykZMyNQDwcadcZFTV7ZL8myRP7O4Xr+++LMkfJ0l3/+SW1S+vqmcmeVI+MTLOSPLY7n7/UZ7jYJKDSXLmbW53Yl8AAJDkFIyMrI5O7E/yuiMtrKpHJXliks9Pcrskp61/trriaIGRJN19KMmhJDn79HP60x8ZANjulLwm42iq6oFJXpbk1Um+IatTKE9Jcvq2VW84yaMBANucikcy3pbVNRYPTfKubcsenOS9W0+ZVNU9T+JsAMAxOuUio7uvq6qLkvx0VR3O6sLPOya5f5J3Jjm3qh6T1TUaD0vy6MWGBQCO6pSLjLUnJ7kmq4s5757k/Ule2t2/VFXPSfL8JJ+R5PeTPC3JLy40JwBwFNW9t697PPv0c/pBn/VNS4+xUW6+7vqlR9hItznrrKVH2Ejv+c7zlx5h4xy+497+vX682tc2Hpe//rEfeWt3HzjSso268BMA2BwiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBH7lh5gaX3Tx/Kxq/5+6THYAz52+PDSI2ykc9947dIjbJxz/9PlS4+wkb7ns9+w9Agb6UE/dvRljmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwYt/SAyyhqg4mOZgkZ+ashacBgN1pTx7J6O5D3X2guw+cnv1LjwMAu9KejAwAYJ7IAABG7NrIqKofqKq3Lz0HAOxVuzYyktwpyb2XHgIA9qpdGxnd/fTurqXnAIC9atdGBgCwLJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDAiH1LDwDwqfRb/3LpETbOlY+629IjbKQnvOhblx5hQ/3MUZc4kgEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMAIkQEAjBAZAMCIjYmMqnpSVV2+9BwAwLHZmMgAADbLCYmMqrp9Vd3hRGxrB895TlWdeTKfEwA4dscdGVV1WlU9rKp+PcmVSb5sff/ZVXWoqj5QVddV1R9U1YEtj7uwqq6vqodW1aVVdUNVvaGqztu2/R+tqivX6740ye22jfCIJFeun+vBx/s6AIAZO46Mqvriqnp2kvckeXmSG5I8PMnFVVVJfjfJuUkemeTLk1yc5PVVddctm9mf5MlJHp/kQUnukOQFW57jW5L8VJIfT3K/JO9I8sPbRvm1JN+W5DOTvKaqLquqp22PFQBgGccUGVV1x6r6oap6a5L/k+QLkzwhyV26+7u7++Lu7iQXJLlvkkd1959092Xd/dQkf53ksVs2uS/J96/X+fMkz03ykHWkJMkTk7yku1/Y3e/s7mck+ZOtM3X3Td39e9396CR3SfLM9fO/q6reWFWPr6rtRz9ueT0Hq+qSqrrkxhw+ll0AAOzQsR7J+MEkFyX5SJIv6O5v7O7f7O6PbFvv/knOSvLB9WmO66vq+iT3SXKvLesd7u53bLn9viRnJPms9e3zk/zxtm1vv/1x3X1td7+4uy9I8hVJPjvJLyd51FHWP9TdB7r7wOnZ/yleNgBwvPYd43qHktyY5F8nubSqXpnkV5O8rrs/tmW92yR5f5KvPsI2rt3yzzdtW9ZbHr9jVbU/q9Mz357VtRp/mdXRkFcdz/YAgE/fMX2od/f7uvsZ3X3vJF+X5PokL0tyRVU9r6ruu171T7M6inDz+lTJ1p8P7GCutyV54Lb7PuF2rXxVVb0wqwtPfz7JZUnu39336+6LuvuaHTwnAHAC7fjIQXe/ubu/N8ldszqN8gVJ3lJVX53ktUnelORVVfX1VXVeVT2oqn5ivfxYXZTkcVX13VX1T6vqyUkesG2db0/y+0lun+TRST6nu/9td1+609cEAJx4x3q65JN09+Ekr0jyiqq6c5KPdXdX1SOy+suQFyW5c1anT96U5KU72PbLq+rzkjwjq2s8fivJzya5cMtqr8vqwtNrP3kLAMDSavVHIXvX7euf9APqoUuPARzNx//ojGO179y7LT3CRrr6Rb7f8Xj8ycN/5q3dfeBIy3ytOAAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwQmQAACNEBgAwYt/SAwB8St1LT7BxbrrivUuPsJFu//VLT7D7OJIBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDACJEBAIwQGQDAiH1LD7CEqjqY5GCSnJmzFp4GAHanPXkko7sPdfeB7j5wevYvPQ4A7Ep7MjIAgHkiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBEiAwAYITIAgBHV3UvPsKiq+mCSv116jqO4U5Krlh5iA9lvO2efHR/77fjYbzt3Ku+ze3b3OUdasOcj41RWVZd094Gl59g09tvO2WfHx347Pvbbzm3qPnO6BAAYITIAgBEi49R2aOkBNpT9tnP22fGx346P/bZzG7nPXJMBAIxwJAMAGCEyAIARIgMAGCEyAIARIgMAGPH/AMENDisXT3r4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('我有一只猫')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
